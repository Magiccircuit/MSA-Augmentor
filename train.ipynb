{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5Config, AutoConfig, T5ForConditionalGeneration\n",
    "\n",
    "from model.msa_shlab import MSAT5,T5Stack\n",
    "from typing import Sequence, Tuple, List, Union\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, RandomSampler, Dataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# textokenizer=T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.msa_dataset import MSADataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Processing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "import itertools\n",
    "from typing import Sequence, Tuple, List, Union\n",
    "import string\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSADataSet(Dataset):\n",
    "    def __init__(self,data_path,seq_per_msa=3,num_files=1000):\n",
    "        deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "        deletekeys[\".\"] = None\n",
    "        deletekeys[\"*\"] = None\n",
    "        self.translation = str.maketrans(deletekeys)\n",
    "        \n",
    "        data_path_list=glob.glob(data_path+'/*/*.a3m')[:num_files]\n",
    "        msa_data=[self.read_msa(data_path,seq_per_msa*2) for data_path in data_path_list]\n",
    "        # filter out msa dosen't meet seq_per_msa requirement\n",
    "        msa_data=[i for i in msa_data if (len(i)==seq_per_msa*2 and self.check_len(i))]\n",
    "        self.src = [msa[:seq_per_msa]  for msa in msa_data]\n",
    "        self.tgt = [msa[seq_per_msa:]  for msa in msa_data]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return {\"src\":self.src[index],\"tgt\":self.tgt[index]}\n",
    "        # return self.msa_data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def remove_insertions(self,sequence) :\n",
    "        \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "        return sequence.translate(self.translation)\n",
    "    def read_msa(self,filename, nseq) :\n",
    "        \"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\"\"\"\n",
    "        return [(record.description, self.remove_insertions(str(record.seq)))\n",
    "                    for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)]\n",
    "    def check_len(self,msa):\n",
    "        #check if all sequence in a msa has the same length\n",
    "        l=set([len(x[1]) for x in msa])\n",
    "        return len(l)==1\n",
    "\n",
    "\n",
    "config=T5Config.from_pretrained('config/')\n",
    "tokenizer=T5Tokenizer.from_pretrained('config/')\n",
    "seq_per_msa=15\n",
    "config=T5Config.from_pretrained('./config/')\n",
    "tokenizer=T5Tokenizer.from_pretrained('./config/')\n",
    "epochs=50\n",
    "data_path='/user/sunsiqi/zl/T5/CASP_msa'\n",
    "train_dataset=MSADataSet(data_path,num_files=10000,seq_per_msa=seq_per_msa)\n",
    "# eval_dataset=MSADataSet(data_path,num_files=100,seq_per_msa=seq_per_msa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSADataSet(Dataset):\n",
    "    def __init__(self,data_path=None,src_seq=None,num_files=None,total_seq=None,recur=True,data_path_list=None):\n",
    "        deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "        deletekeys[\".\"] = None\n",
    "        deletekeys[\"*\"] = None\n",
    "        self.translation = str.maketrans(deletekeys)\n",
    "        if data_path_list is None:\n",
    "            if num_files is not None:\n",
    "                if recur:\n",
    "                    data_path_list=glob.glob(data_path+'/*/*.a3m')[:num_files]\n",
    "                else:\n",
    "                    data_path_list=glob.glob(data_path+'/*.a3m')[:num_files]\n",
    "            else:\n",
    "                # logger.warning('Train on all msa data within given data path')\n",
    "                if recur:\n",
    "                    data_path_list=glob.glob(data_path+'/*/*.a3m')\n",
    "                else:\n",
    "                    data_path_list=glob.glob(data_path+'/*.a3m')\n",
    "                    print(data_path_list)\n",
    "        msa_data=[self.read_msa(data_path,total_seq) for data_path in data_path_list]\n",
    "        # filter out msa dosen't meet seq_per_msa requirement\n",
    "        msa_data=[i for i in msa_data if (len(i)==total_seq and self.check_len(i))]\n",
    "        self.src = [msa[:src_seq]  for msa in msa_data]\n",
    "        self.tgt = [msa[src_seq:]  for msa in msa_data]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return {\"src\":self.src[index],\"tgt\":self.tgt[index]}\n",
    "        # return self.msa_data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def remove_insertions(self,sequence) :\n",
    "        \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "        return sequence.translate(self.translation)\n",
    "    def read_msa(self,filename, nseq) :\n",
    "        \"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\"\"\"\n",
    "        return [(record.description, self.remove_insertions(str(record.seq)))\n",
    "                    for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)]\n",
    "    def check_len(self,msa):\n",
    "        #check if all sequence in a msa has the same length\n",
    "        l=set([len(x[1]) for x in msa])\n",
    "        return len(l)==1\n",
    "dataset=MSADataSet('/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/src_3_all_13/test/',src_seq=3,num_files=None,total_seq=13,recur=False)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=MSADataSet(src_seq=3,num_files=None,total_seq=13,recur=False,data_path_list=['/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/src_3_all_13/total/T1046s2-D1_all.a3m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]['src'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "src_c=Counter()\n",
    "tgt_c=Counter()\n",
    "for msa in dataset:\n",
    "    for seq in msa['src']:\n",
    "        # print(seq[1],len(seq[1]))\n",
    "        text=seq[1]\n",
    "        # print(Counter(text))\n",
    "        src_c+=Counter(text)\n",
    "    for seq in msa['tgt']:\n",
    "        text=seq[1]\n",
    "        tgt_c+=Counter(text)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "src_c=sorted(src_c,key=lambda x:x[1],reverse=True)\n",
    "tgt_c=sorted(tgt_c,key=lambda x:x[1],reverse=True)\n",
    "labels, values =zip(*src_c)\n",
    "indexes = np.arange(len(labels))\n",
    "plt.xticks(indexes , labels)\n",
    "plt.bar(indexes, values)\n",
    "plt.xlabel('class')\n",
    "plt.ylabel('number')\n",
    "# plt.title(\" {} MSA's src files({} sequences)\".format(len(dataset),seq_per_msa))\n",
    "plt.show()\n",
    "\n",
    "labels, values =zip(*tgt_c)\n",
    "indexes = np.arange(len(labels))\n",
    "plt.bar(indexes, values)\n",
    "plt.xticks(indexes , labels)\n",
    "plt.xlabel('class')\n",
    "plt.ylabel('number')\n",
    "# plt.title(\"  {} MSA's tgt files({} sequences)\".format(len(train_dataset),seq_per_msa))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RawMSA = Sequence[Tuple[str, str]]\n",
    "class BatchConverter(object):\n",
    "    \"\"\"Callable to convert an unprocessed (labels + strings) batch to a\n",
    "    processed (labels + tensor) batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer,max_len=512):\n",
    "        self.max_len=max_len-1\n",
    "        self.tokenizer = tokenizer\n",
    "    def __call__(self, raw_batch: Sequence[Tuple[str, str]]):\n",
    "        # RoBERTa uses an eos token, while ESM-1 does not.\n",
    "        batch_size = len(raw_batch)\n",
    "        batch_labels, seq_str_list = zip(*raw_batch)\n",
    "        seq_encoded_list = [self.tokenizer(self._tokenize(seq_str),truncation=True,max_length=self.max_len+1).input_ids for seq_str in seq_str_list]\n",
    "        max_len = max(len(seq_encoded) for seq_encoded in seq_encoded_list)\n",
    "        tokens = torch.empty(\n",
    "            (\n",
    "                batch_size,\n",
    "                max_len \n",
    "            ),\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "        tokens.fill_(self.tokenizer.pad_token_id)\n",
    "        labels = []\n",
    "        strs = []\n",
    "        for i, (label, seq_str, seq_encoded) in enumerate(\n",
    "            zip(batch_labels, seq_str_list, seq_encoded_list)\n",
    "        ):\n",
    "            labels.append(label)\n",
    "            strs.append(seq_str)\n",
    "            seq = torch.tensor(seq_encoded, dtype=torch.int64)\n",
    "            tokens[i,0:len(seq_encoded)] = seq\n",
    "\n",
    "        return labels, strs, tokens\n",
    "    def _tokenize(self,sequence):\n",
    "        return ' '.join(list(sequence)) \n",
    "class DataCollatorForMSA(BatchConverter):\n",
    "    def msa_batch_convert(self, inputs: Union[Sequence[RawMSA], RawMSA]):\n",
    "        # RawMSA: Sequence[label:str,acid_seq:str]\n",
    "        if isinstance(inputs[0][0], str):\n",
    "            # Input is a single MSA\n",
    "            raw_batch: Sequence[RawMSA] = [inputs]  # type: ignore\n",
    "        else:\n",
    "            raw_batch = inputs  # type: ignore\n",
    "\n",
    "        batch_size = len(raw_batch)\n",
    "        max_alignments = max(len(msa) for msa in raw_batch) #MSA的数量\n",
    "        max_seqlen_msa = max(len(msa[0][1]) for msa in raw_batch) # MSA的每个序列长度\n",
    "        max_seqlen=min(max_seqlen_msa,self.max_len)+1 #加一是为了凑齐每句话结尾有一个/s\n",
    "        tokens = torch.empty(\n",
    "            (\n",
    "                batch_size,\n",
    "                max_alignments,\n",
    "                max_seqlen,\n",
    "            ),\n",
    "            dtype=torch.int64,\n",
    "        )\n",
    "        tokens.fill_(self.tokenizer.pad_token_id)\n",
    "        labels = []\n",
    "        strs = []\n",
    "\n",
    "        for i, msa in enumerate(raw_batch):\n",
    "            msa_seqlens = set(len(seq) for _, seq in msa)\n",
    "            if not len(msa_seqlens) == 1:\n",
    "                raise RuntimeError(\n",
    "                    \"Received unaligned sequences for input to MSA, all sequence \"\n",
    "                    \"lengths must be equal.\"\n",
    "                )\n",
    "            msa_labels, msa_strs, msa_tokens = super().__call__(msa)\n",
    "            msa_len=msa_tokens.size(1)\n",
    "            msa_tokens=msa_tokens[:,:min(msa_len,max_seqlen)]\n",
    "            labels.append(msa_labels)\n",
    "            strs.append(msa_strs)\n",
    "            tokens[i, : msa_tokens.size(0), : msa_tokens.size(1)] = msa_tokens\n",
    "        return tokens\n",
    "    def __call__(self,batch): \n",
    "        input_ids=self.msa_batch_convert([example[\"src\"] for example in batch])\n",
    "        attention_mask=input_ids.ne(self.tokenizer.pad_token_id).type_as(input_ids)\n",
    "        labels=self.msa_batch_convert([example[\"tgt\"] for example in batch])\n",
    "        decoder_attention_mask=labels.ne(self.tokenizer.pad_token_id).type_as(input_ids)\n",
    "        labels[labels==self.tokenizer.pad_token_id]=-100\n",
    "        # labels[labels==128]=-100\n",
    "        return {'input_ids':input_ids,'labels':labels,\"attention_mask\":attention_mask,\"decoder_attention_mask\":decoder_attention_mask}\n",
    "msadata_collator=DataCollatorForMSA(tokenizer,max_len=512)\n",
    "batch_size=2\n",
    "msa_dataloader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=msadata_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_dataset:\n",
    "    fl=[]\n",
    "    for i in i['src']:\n",
    "        fl.append(i[0])\n",
    "        fl.append(i[1])\n",
    "\n",
    "    print (fl)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MSAT5(config).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src=train_dataset[0]['src']\n",
    "src=msadata_collator.msa_batch_convert(src).to('cuda:0')\n",
    "src.size(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model.generate(src,do_sample=True,top_k=5,top_p=0.95,max_length=src.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output[0][0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src,src.shape)\n",
    "output=model.generate(src)\n",
    "print(output.shape)\n",
    "for i in output[0]:\n",
    "    print(tokenizer.decode(i))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]['src'][0],src,src.shape,tokenizer.decode(src[0][0],skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=tokenizer.decode(src[0][0],skip_special_tokens=True).replace(' ','')\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试encoder decoder的extended_attention_mask的创建异同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "config.axial_attention=True\n",
    "decoder_config = copy.deepcopy(config)\n",
    "decoder_config.is_decoder=True\n",
    "decoder=T5Stack(decoder_config)\n",
    "encoder_config=copy.deepcopy(config)\n",
    "encoder=T5Stack(encoder_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_input_ids=msa_batch_converter.msa_batch_convert(msa_dataset[0]['src'])[0][:2,100:105]\n",
    "attention_mask=msa_input_ids.ne(tokenizer.pad_token_id)\n",
    "msa_input_ids.shape,attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.is_decoder=False\n",
    "extended_attention_mask_decoder=decoder.get_extended_attention_mask(attention_mask,msa_input_ids.shape,'cpu')\n",
    "extended_attention_mask_encoder=encoder.get_extended_attention_mask(attention_mask,msa_input_ids.shape,'cpu')\n",
    "print('-'*20,'input','-'*20)\n",
    "print('input_ids shape: {}\\ninput_ids:\\n{}'.format(msa_input_ids.shape,msa_input_ids))\n",
    "print('-'*20,'for decoder','-'*20)\n",
    "print('decoder extended attention mask shape: {}\\ndecoder extended attention mask value:\\n{}'.format(extended_attention_mask_decoder.shape,extended_attention_mask_decoder))\n",
    "print('-'*20,'for encoder','-'*20)\n",
    "print('encoder extended attention mask shape: {}\\nencoder extended attention mask value:\\n{}'.format(extended_attention_mask_encoder.shape,extended_attention_mask_encoder))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate时，input_ids为1，则生成非caulsal的(类似encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder.get_extended_attention_mask(attention_mask,torch.randint(10,(2,1)).shape,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class MSADataSet(Dataset):\n",
    "    def __init__(self,data_path,num_msa_files=None,num_alignments=3):\n",
    "        deletekeys = dict.fromkeys(string.ascii_lowercase)\n",
    "        deletekeys[\".\"] = None\n",
    "        deletekeys[\"*\"] = None\n",
    "        src_seq_per_msa='random'\n",
    "        src_seq_per_msa_l=2\n",
    "        src_seq_per_msa_u=10\n",
    "        total_seq_per_msa=30\n",
    "        self.translation = str.maketrans(deletekeys)\n",
    "        if num_msa_files is not None:\n",
    "            # train on small dataset\n",
    "            data_path_list=glob.glob(data_path+'/*/*.a3m')[:num_msa_files]\n",
    "        else:\n",
    "            # train on full dataset\n",
    "            data_path_list=glob.glob(data_path+'/*/*.a3m')\n",
    "        \n",
    "        msa_data=[self.read_msa(data_path,total_seq_per_msa) for data_path in data_path_list]\n",
    "        msa_data=[i for i in msa_data if (len(i)==total_seq_per_msa and self.check_len(i))]\n",
    "        print(len(msa_data))\n",
    "        self.src = [msa[:src_seq_per_msa if isinstance(src_seq_per_msa, int) else random.randint(src_seq_per_msa_l,src_seq_per_msa_u)] for msa in msa_data]\n",
    "        print(len(self.src))\n",
    "        tgt_seq_num_list=[len(src) for src in self.src]\n",
    "        self.tgt = [msa[tgt_seq_per_msa:] for msa,tgt_seq_per_msa in zip(msa_data,tgt_seq_num_list)]     \n",
    "    def __getitem__(self, index):\n",
    "        return {\"src\":self.src[index],\"tgt\":self.tgt[index]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def remove_insertions(self,sequence) :\n",
    "        \"\"\" Removes any insertions into the sequence. Needed to load aligned sequences in an MSA. \"\"\"\n",
    "        return sequence.translate(self.translation)\n",
    "    def read_msa(self,filename, nseq) :\n",
    "        \"\"\" Reads the first nseq sequences from an MSA file, automatically removes insertions.\"\"\"\n",
    "        return [(record.description, self.remove_insertions(str(record.seq)))\n",
    "                    for record in itertools.islice(SeqIO.parse(filename, \"fasta\"), nseq)]\n",
    "    def check_len(self,msa):\n",
    "        #check if all sequence in a msa has the same length\n",
    "        l=set([len(x[1]) for x in msa])\n",
    "        return len(l)==1\n",
    "data_path='dataset/'\n",
    "msa_dataset=MSADataSet(data_path,num_msa_files=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=int(0.2*len(msa_dataset))\n",
    "test_size=len(msa_dataset)-train_size\n",
    "train,test=torch.utils.data.random_split(msa_dataset,[train_size,test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(msa_dataset)):\n",
    "    print(len(msa_dataset[i]['src']),len(msa_dataset[i]['tgt']))\n",
    "    assert len(msa_dataset[i]['src'])+len(msa_dataset[i]['tgt'])==30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "l=CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(3,2,205,130)\n",
    "b=torch.randint(130,(3,2,205))\n",
    "b=torch.argmax(a,-1)\n",
    "c=torch.randint(10,(2,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(b==torch.argmax(a,-1))/(b.size(0)*b.size(1)*b.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l(a.view(-1,130),b.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl=0\n",
    "for i,j in zip(a,b):\n",
    "    print(i.shape,j.shape)\n",
    "    lo=l(i.view(-1,130),j.view(-1))\n",
    "    print(lo)\n",
    "    tl+=lo\n",
    "print(tl/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(c).item()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*75)\n",
    "print('|',' '*29,'New forward step',' '*24,'|')\n",
    "print('-'*75)\n",
    "a=torch.randn(2,43,4,5,6)\n",
    "print('| %-.25s'%'dsaddsdssadasdsainput_ids shape is:','%47s'%'{}|'.format(a.shape if a is not None else None))\n",
    "print('| %-25s'%' is:','%47s'%'{}|'.format(c.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试logitsProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation_logits_process import LogitsProcessorList,TopKLogitsWarper,TopPLogitsWarper\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k=5\n",
    "top_p=0.92\n",
    "num_beams=1\n",
    "warpers = LogitsProcessorList()\n",
    "warpers.append(TopKLogitsWarper(top_k=top_k, min_tokens_to_keep=(2 if num_beams > 1 else 1)))\n",
    "warpers.append(TopPLogitsWarper(top_p=top_p, min_tokens_to_keep=(2 if num_beams > 1 else 1)))\n",
    "logits=torch.randn(2,2,1,130)\n",
    "next_token_logits = logits[:,:, -1, :]\n",
    "next_token_scores=next_token_logits\n",
    "next_token_scores = warpers(input_ids, next_token_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOPK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.randn(2,4,1,130)\n",
    "b=torch.randint(10,(2,3))\n",
    "b,torch.topk(b,2),torch.topk(b,2)[0],torch.topk(b,2)[0][...,-1,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=a < torch.topk(a, 5)[0][..., -1, None]\n",
    "indices[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=torch.randn(2,2,5)\n",
    "sorted_logits, sorted_indices = torch.sort(scores, descending=True)\n",
    "print('sorted_indices: \\n',sorted_indices,sorted_indices.shape)\n",
    "cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
    "print('cumulative_probs: \\n',cumulative_probs,cumulative_probs.shape)\n",
    "sorted_indices_to_remove = cumulative_probs > 0.92\n",
    "print('sorted_indices_to_remove: \\n',sorted_indices_to_remove,sorted_indices_to_remove.shape)\n",
    "sorted_indices_to_remove[..., 0] = 0\n",
    "indices_to_remove = torch.stack([i.scatter(1, j, i) for (i,j) in zip(sorted_indices_to_remove,sorted_indices)])\n",
    "print('indices_to_remove: \\n',indices_to_remove,indices_to_remove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "probs = nn.functional.softmax(scores, dim=-1)\n",
    "torch.stack([torch.multinomial(i,num_samples=1) for i in probs ]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2,2,1)[:,:,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=torch.randn(1,10,10)+10\n",
    "next_tokens=torch.stack([torch.multinomial(i,num_samples=1) for i in probs ],dim=0)\n",
    "next_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(probs[0],num_samples=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a=torch.tensor([1,2,3,2,5])\n",
    "mask=(a!=2) & (a!=3)\n",
    "a[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casp14_fasta_path='/share/wangsheng/train_test_data/CASP_RawData/CASP14_RawData/CASP14DM_SEQ/'\n",
    "casp14_name_list=[file.split('.')[0] for file in os.listdir(casp14_fasta_path)]\n",
    "all_msa_path='/user/sunsiqi/zl/T5/CASP_msa/allDM_msa/'\n",
    "for file_name in casp14_name_list:\n",
    "    file_path=all_msa_path+file_name+'.a3m'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "a=SeqIO.parse('/user/sunsiqi/zl/T5/CASP_msa/allDM_msa/T1093-D3.a3m', \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/user/sunsiqi/zl/T5/CASP_msa/allDM_msa/T1093-D3.a3m','r') as f:\n",
    "    context=f.readlines()[:4]\n",
    "    print(context,len(context))\n",
    "    print(\"\".join(context))\n",
    "    with open('/user/sunsiqi/zl/T5/AF2TEST/x.a3m','w') as fw:\n",
    "        fw.write(\"\".join(context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = \"/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/src_3_all_13/source\"\n",
    "for dir in sorted(os.listdir(pred)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pred = \"/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/src_3_all_13/source\"\n",
    "for dir in sorted(os.listdir(pred)):\n",
    "    print(dir.split('_')[0])\n",
    "    dsa\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a='AF2TEST/CASP14/src_3_all_13/generate_10'\n",
    "b=a.split('/')\n",
    "b.insert(2,'output')\n",
    "b=os.path.join(*b)+'/'\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_msa_dir='/user/sunsiqi/zl/T5/AF2TEST/CASP14/src_3_all_13/generate_10/'\n",
    "pdboutdir=generated_msa_dir.split('/')\n",
    "print(pdboutdir)\n",
    "pdboutdir.insert(7,'output')\n",
    "pdboutdir=os.path.join(*pdboutdir)\n",
    "pdboutdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_iddt(pred,outdir):\n",
    "    lddt = \"/share/wangsheng/GitBucket/Fast_lDDT/Fast_lDDT\"\n",
    "    seqdir = \"/share/wangsheng/train_test_data/CASP_RawData/allDM_SEQ/\"\n",
    "    native = \"/share/wangsheng/train_test_data/CASP_RawData/allDM_Native/\"\n",
    "    \n",
    "    # outdir = \"pred-lddt-iter3-log/\"\n",
    "    # outdir=\"/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/src_3_all_13/pred-lddt/\"\n",
    "    #pred = \"/share/liyu/hl/fold-result-tmp/tmp_res_casp13_fasta/1000000/\"\n",
    "    # pred = \"/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/src_3_all_13/generate_10/\"\n",
    "\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    with open(os.path.join(outdir,\"{}.csv\".format(pred.split('/')[-2])), 'w+') as fp:\n",
    "        fp.write(\"name,result\\n\")\n",
    "        for dir in sorted(os.listdir(pred)):\n",
    "            dir_noext=dir.split('_')[0]\n",
    "            p = subprocess.Popen(\n",
    "                [lddt, \n",
    "                '-i', os.path.join(seqdir, \"%s.seq\"%dir_noext), \n",
    "                '-n', os.path.join(native, \"%s.pdb\"%dir_noext), \n",
    "                '-m', os.path.join(pred, dir, 'ranked_0.pdb'),\n",
    "                '-v', '1'],\n",
    "                #shell=True, \n",
    "                stdout=subprocess.PIPE, \n",
    "                stderr=subprocess.STDOUT\n",
    "            )\n",
    "            res = p.stdout.readlines()[-1].decode().split(' ')[2]\n",
    "            fp.write(\"%s,%s\\n\"%(dir, res))\n",
    "            ret = p.wait()\n",
    "    print(\"finish %s\"%(dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "eval_iddt(outdir='/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/src_5_all_15/pred-lddt/source_sorted/',pred='/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/src_5_all_15/source_sorted/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试多轮增强"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir='/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/msa_l1_u50/predict/Gtime08-19-10:48_Rpen1_Gtimes1_f_0/'\n",
    "\n",
    "Gsteps=os.listdir(result_dir)\n",
    "#target {t1026:gstep1;t1034:gstep2}\n",
    "caspfile_score={}\n",
    "caspfiles=os.listdir(os.path.join(result_dir,Gsteps[0]))\n",
    "keys=[example for example in caspfiles]\n",
    "for key in keys:\n",
    "    caspfile_score.update({key:[]})\n",
    "for gstep in Gsteps:\n",
    "    Gstep_path=os.path.join(result_dir,gstep)\n",
    "    for caspfile in caspfiles:\n",
    "        caspfile_ranking_path=os.path.join(Gstep_path,caspfile,'ranking_debug.json')\n",
    "        scores=json.load(open(caspfile_ranking_path,'r'))\n",
    "        score=scores['plddts'][scores['order'][0]]\n",
    "        caspfile_score[caspfile].append((gstep,score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "caspfile_step={}\n",
    "def highest_gstep(g_scores):\n",
    "    gsteps=[x[0] for x in g_scores]\n",
    "    scores=np.array([x[1] for x in g_scores])\n",
    "    idx=np.argmax(scores)\n",
    "    return gsteps[idx]\n",
    "for key in caspfile_score:\n",
    "    g_scores=caspfile_score[key]\n",
    "    caspfile_step[key]=highest_gstep(g_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_file_path(caspfile_step,result_dir):\n",
    "    input_dir=result_dir.replace('output','input')\n",
    "    highest_collection_path=[os.path.join(input_dir,gstep,caspfile+'.a3m') for caspfile,gstep in caspfile_step.items()]\n",
    "    for path in highest_collection_path:\n",
    "        assert os.path.exists(path)\n",
    "    return highest_collection_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_file_path(caspfile_step,result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_1/T1093-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_3/T1068-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_1/T1038-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_0/T1099-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_4/T1026-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_2/T1082-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_3/T1064-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_3/T1030-D2_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_1/T1085-D3_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_2/T1037-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_3/T1085-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_1/T1094-D2_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_0/T1096-D2_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_1/T1074-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_2/T1038-D2_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_3/T1060s2-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_4/T1046s1-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_1/T1041-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_2/T1046s2-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_0/T1100-D2_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_4/T1096-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_0/T1042-D1_generate.a3m',\n",
       " '/user/sunsiqi/zl/T5/AF2TEST/CASP14/input/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0/Gstep_3/T1093-D3_generate.a3m']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_best_generation(result_dir):\n",
    "    Gsteps=os.listdir(result_dir)\n",
    "    caspfile_score={}\n",
    "    caspfiles=os.listdir(os.path.join(result_dir,Gsteps[0]))\n",
    "    keys=[example for example in caspfiles]\n",
    "    for key in keys:\n",
    "        caspfile_score.update({key:[]})\n",
    "    for gstep in Gsteps:\n",
    "        Gstep_path=os.path.join(result_dir,gstep)\n",
    "        for caspfile in caspfiles:\n",
    "            caspfile_ranking_path=os.path.join(Gstep_path,caspfile,'ranking_debug.json')\n",
    "            try:\n",
    "                scores=json.load(open(caspfile_ranking_path,'r'))\n",
    "                score=scores['plddts'][scores['order'][0]]\n",
    "                caspfile_score[caspfile].append((gstep,score))\n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "           \n",
    "    caspfile_step={}\n",
    "    def highest_gstep(g_scores):\n",
    "        gsteps=[x[0] for x in g_scores]\n",
    "        scores=np.array([x[1] for x in g_scores])\n",
    "        idx=np.argmax(scores)\n",
    "        return gsteps[idx]\n",
    "    for key in caspfile_score:\n",
    "        g_scores=caspfile_score[key]\n",
    "        caspfile_step[key]=highest_gstep(g_scores)\n",
    "    input_dir=result_dir.replace('output','input')\n",
    "    highest_collection_path=[os.path.join(input_dir,gstep,caspfile+'.a3m') for caspfile,gstep in caspfile_step.items()]\n",
    "    for path in highest_collection_path:\n",
    "        assert os.path.exists(path)\n",
    "    return highest_collection_path\n",
    "fetch_best_generation('/user/sunsiqi/zl/T5/AF2TEST/CASP14/output/msa_l1_u50/predict/Gtime08-17-08:50_Rpen1_Gtimes5_f0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9dd394f4523f21bb3e24cf40784331b0c0d90f8d11b04768ccd0b3977e7b422e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('esm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
